{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAvgränsningar\\nJag väljer att inte gå in på djupet på filformatens arkitekturer, då endast en\\ndjupgående genomgång av en fil förtjänar en uppsats för sig själv.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Website at https://parquet.apache.org/\n",
    "\"\"\"\n",
    "29-4-2022\n",
    "Apache Parquet är ett open source, kolumn orienterat file format som är designat för data förvaring och hämtning.\n",
    "Den har effektiv datakompression och kodning scheman med förbättrad prestanda att hantera komplex data i omgångar.\n",
    " \n",
    "Parquet finns tillgänglig till språken Java, C++, Python med mera.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "https://arrow.apache.org/docs/python/getstarted.html\n",
    "Apache arrow är en kombination av verktyg för att jobba med olika filtyper inom bigdata.\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "För att utföra testerna skulle man kunna generera skräpdata utan någon mening.\n",
    "Jag väljer att använda riktiga dataset för att få en bra representation hur resultaten ser ut i verkligheten.\n",
    "Två datasets kommer att användas, ett smalt dataset med få kolumner och ett brett dataset med många kolumner.\n",
    " \n",
    "Formatet smalt mot brett är för att täcka det fallen där ett filformat kanske är bättre på att hantera\n",
    "få antal kolumner mot många, vice versa.\n",
    " \n",
    "Official Reddit r/place Dataset CSV:\n",
    "https://www.kaggle.com/datasets/antoinecarpentier/redditrplacecsv\n",
    " \n",
    "Pratar om benchmarks\n",
    "https://www.slideshare.net/HadoopSummit/file-format-benchmark-avro-json-orc-and-parquet\n",
    "\n",
    "Github Logs:\n",
    "\n",
    "\n",
    "Skillnaded mellan Parquet och Feather.\n",
    "https://stackoverflow.com/questions/48083405/what-are-the-differences-between-feather-and-parquet\n",
    "Går igenom timeit också.\n",
    "\n",
    "timeit för att mäta tid på funktioner.\n",
    "https://stackoverflow.com/a/55239060\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "# Do something\n",
    "end = timer()\n",
    "time_in_seconds = end - start\n",
    "\n",
    "\n",
    "Försökte att använda %%timeit på funktionerna\n",
    "https://docs.python.org/3/library/timeit.html\n",
    "\"By default, timeit() temporarily turns off garbage collection during the timing.\"\n",
    "går att fixa med:\n",
    " - timeit.Timer('for i in range(10): oct(i)', 'gc.enable()').timeit()\n",
    "\n",
    "\n",
    "Steam Reviews Dataset: https://www.kaggle.com/datasets/najzeko/steam-reviews-2021\n",
    " - 21 Miljoner användarrecensioner av 300 spel på spelplattformen Steam.\n",
    " - 21 Miljoner rader\n",
    " - 23 Kolumner\n",
    " - 8.17 GB Stor\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "Avgränsningar\n",
    "Jag väljer att inte gå in på djupet på filformatens arkitekturer, då endast en\n",
    "djupgående genomgång av en fil förtjänar en uppsats för sig själv.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "import pyarrow.parquet as parquet\n",
    "import pyarrow.orc as orc\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import gc\n",
    "#days = pa.array([1, 12, 17, 23, 28], type=pa.int8())\n",
    "file = None\n",
    "reddit_place = \"2022_place_canvas_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file read.\n"
     ]
    }
   ],
   "source": [
    "# Getting raw data and transform to different formats\n",
    "# Manually making sure that file is deleted from memory if rerunning cell.\n",
    "if file:\n",
    "    del file\n",
    "    # Garbage collecion: https://docs.python.org/3/library/gc.html\n",
    "    gc.collect()\n",
    "\n",
    "    print('Clearing memory')\n",
    "    time.sleep(2)\n",
    "\n",
    "print('Starting file read.')\n",
    "start = timer()\n",
    "file = csv.read_csv(f'data/original/{reddit_place}.csv')\n",
    "end = timer()\n",
    "total_in_seconds = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have data in pyarrow table, so we can save it to parquet.\n",
    "parquet.write_table(file, 'data/raw/parquet/')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1014a147ebe84edf89cc949d1474b378add343e44159593a1f5b609a8f202a90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
